name: ft_transformer
d_embedding: 400
model_path:
use_mlp_head: false
freeze_feature_extractor: false
token_bias: true
n_layers: 3
n_heads: 8
d_ffn_factor: 1.2020972200730662
attention_dropout: 0.21033034903406572
ffn_dropout: 0.1
residual_dropout: 0.08463849011414637
activation: reglu
prenormalization: true
initialization: kaiming
kv_compression:
kv_compression_sharing: