name: ft_transformer
d_embedding: 152
model_path:
use_mlp_head: false
freeze_feature_extractor: false
token_bias: true
n_layers: 3
n_heads: 8
d_ffn_factor: 2.3597
attention_dropout: 0.08796
ffn_dropout: 0.08796
residual_dropout: 0.0
activation: reglu
prenormalization: true
initialization: kaiming
kv_compression:
kv_compression_sharing: